# test_basic_comparison.py
import os
import random
import numpy as np
import torch
import torch.nn as nn

# ğŸ’¡ Step 1 í™˜ê²½ (Basic)
from library_env_random_start import LibraryShelfEnv

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ğŸ’¡ í•™ìŠµëœ ëª¨ë¸ íŒŒì¼
MODEL_PATH = "library_shelf_random_start_curriculum.pt"


# -------------------------------------------------------------
# 1. ëª¨ë¸ êµ¬ì¡° (train ì½”ë“œì™€ ë™ì¼í•´ì•¼ í•¨)
# -------------------------------------------------------------
class DQN(nn.Module):
    def __init__(self, state_dim, n_actions):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, n_actions)
        )

    def forward(self, x):
        return self.net(x)


# -------------------------------------------------------------
# 2. ëª¨ë¸ ë¡œë“œ
# -------------------------------------------------------------
def load_policy(env):
    if not os.path.exists(MODEL_PATH):
        print(f"ğŸš¨ ëª¨ë¸ íŒŒì¼({MODEL_PATH})ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í•™ìŠµì„ ëŒë¦¬ì„¸ìš”!")
        return None

    ckpt = torch.load(MODEL_PATH, map_location=device)

    # í™˜ê²½ì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    dummy_s = env.reset()
    state_dim = len(dummy_s)
    n_actions = 4

    policy = DQN(state_dim, n_actions).to(device)
    policy.load_state_dict(ckpt["state_dict"])
    policy.eval()

    print(f"ğŸ“¦ Step 1 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {MODEL_PATH}")
    return policy


# -------------------------------------------------------------
# 3. í‰ê°€ í•¨ìˆ˜
# -------------------------------------------------------------
def run_test(env, policy, mode_name, random_start, episodes=100):
    print(f"\nğŸ§ª [{mode_name}] í…ŒìŠ¤íŠ¸ ì§„í–‰ ì¤‘ ({episodes}íŒ)...")
    success_count = 0
    total_steps = 0

    for ep in range(episodes):
        # íƒ€ê²Ÿ ëœë¤ ì„¤ì •
        target_idx = random.randint(0, len(env.target_keys) - 1)

        # ì‹œì‘ ìœ„ì¹˜ ì„¤ì • (í•µì‹¬!)
        state = env.reset(target_idx=target_idx, random_start=random_start)

        done = False
        steps = 0

        while not done and steps < env.max_steps:
            s_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
            with torch.no_grad():
                q = policy(s_t)[0]
            action = int(q.argmax().item())

            state, reward, done, info = env.step(action)
            steps += 1

            if info.get("reached_goal", False):
                success_count += 1
                total_steps += steps

    success_rate = success_count / episodes * 100
    avg_steps = total_steps / success_count if success_count > 0 else 0

    print(f"   ê²°ê³¼: ì„±ê³µë¥  {success_rate:.1f}% | í‰ê·  ìŠ¤í… {avg_steps:.1f}")
    return success_rate, avg_steps


# -------------------------------------------------------------
# Main
# -------------------------------------------------------------
if __name__ == "__main__":
    env = LibraryShelfEnv()
    policy = load_policy(env)

    if policy:
        print("=" * 50)
        print("ğŸ“Š Step 1 (Basic) : Vanilla Double DQN ì„±ëŠ¥ ê²€ì¦")
        print("=" * 50)

        # 1. ê³ ì • ìœ„ì¹˜(S) ì¶œë°œ í…ŒìŠ¤íŠ¸
        acc_S, step_S = run_test(env, policy, "S ì¶œë°œ (Fixed)", random_start=False)

        # 2. ëœë¤ ìœ„ì¹˜ ì¶œë°œ í…ŒìŠ¤íŠ¸
        acc_R, step_R = run_test(env, policy, "ëœë¤ ì¶œë°œ (Random)", random_start=True)

        print("\n" + "=" * 50)
        print(f"ğŸ“ ìµœì¢… ìš”ì•½")
        print(f"1. S ì¶œë°œ   : {acc_S:.1f}% (ë‚œì´ë„ í•˜)")
        print(f"2. ëœë¤ ì¶œë°œ: {acc_R:.1f}% (ë‚œì´ë„ ì¤‘)")
        print("=" * 50)

        if acc_R > 90:
            print("âœ… ê²°ë¡ : Basic í™˜ê²½ì€ ê¸°ë³¸ ëª¨ë¸ë¡œë„ ì¶©ë¶„íˆ ì •ë³µ ê°€ëŠ¥í•˜ë‹¤!")
        else:
            print("âš ï¸ ê²°ë¡ : ì•„ì§ í•™ìŠµì´ ë” í•„ìš”í•˜ë‹¤.")