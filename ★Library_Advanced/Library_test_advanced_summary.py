import os
import random
import numpy as np
import torch
import torch.nn as nn

# ğŸ’¡ í™˜ê²½: ìµœì¢… ë³´ìŠ¤ 'Advanced'
from env_advanced import LibraryShelfEnvAG

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ğŸ’¡ ëª¨ë¸: Double Dueling DQN (ìµœì¢… í•™ìŠµ íŒŒì¼)
MODEL_PATH = "library_AG_double_dueling.pt"


# -------------------------------------------------------------
# 1. Dueling DQN êµ¬ì¡° (train_advanced.pyì™€ ë™ì¼)
# -------------------------------------------------------------
class DuelingDQN(nn.Module):
    def __init__(self, state_dim, n_actions):
        super().__init__()
        self.feature = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
        )
        self.value_stream = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
        )
        self.adv_stream = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, n_actions),
        )

    def forward(self, x):
        f = self.feature(x)
        v = self.value_stream(f)
        a = self.adv_stream(f)
        q = v + (a - a.mean(dim=1, keepdim=True))
        return q


# -------------------------------------------------------------
# 2. ëª¨ë¸ ë¡œë“œ
# -------------------------------------------------------------
def load_policy(env):
    if not os.path.exists(MODEL_PATH):
        print(f"ğŸš¨ ëª¨ë¸ íŒŒì¼({MODEL_PATH})ì´ ì—†ìŠµë‹ˆë‹¤! train_advanced.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return None

    ckpt = torch.load(MODEL_PATH, map_location=device)

    # ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° í™•ì¸
    state_dim = env.state_dim
    # envì— n_actionsê°€ ì—†ìœ¼ë©´ 4ë¡œ ì²˜ë¦¬
    n_actions = getattr(env, 'n_actions', 4)

    policy = DuelingDQN(state_dim, n_actions).to(device)

    try:
        if isinstance(ckpt, dict) and 'state_dict' in ckpt:
            policy.load_state_dict(ckpt['state_dict'])
        else:
            policy.load_state_dict(ckpt)

        policy.eval()
        print(f"ğŸ“¦ Final Model ë¡œë“œ ì™„ë£Œ: {MODEL_PATH}")
    except Exception as e:
        print(f"ğŸš¨ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

    return policy


# -------------------------------------------------------------
# 3. ìƒì„¸ í‰ê°€ í•¨ìˆ˜ (S ì‹œì‘ / ëœë¤ ì‹œì‘ ë¶„ë¦¬)
# -------------------------------------------------------------
def eval_mode(env, policy, num_episodes=200, random_start=True):
    """
    random_start=True  : ë§¤ ì—í”¼ì†Œë“œ ëœë¤ ìœ„ì¹˜ + ëœë¤ íƒ€ê²Ÿ
    random_start=False : í•­ìƒ Sì—ì„œ ì‹œì‘ + ëœë¤ íƒ€ê²Ÿ
    """
    mode_name = "ëœë¤ ì‹œì‘" if random_start else "Sì—ì„œ ì‹œì‘"
    print(f"\n==================================")
    print(f"ğŸ¬ [Final Model | {mode_name}] ì„±ëŠ¥ í‰ê°€ ({num_episodes}íšŒ)")
    print(f"==================================")

    target_keys = env.target_keys

    # íƒ€ê²Ÿë³„ í†µê³„ ì €ì¥ì†Œ
    per_target = {
        k: {
            "total": 0, "success": 0, "steps": 0, "success_steps": 0, "fail_steps": 0,
            "wall_hit_episodes": 0, "stuck_episodes": 0, "timeout_episodes": 0,
        }
        for k in target_keys
    }

    # ì „ì²´ í†µê³„ ë³€ìˆ˜
    total_success = 0
    total_steps = 0
    total_success_steps = 0
    total_fail_steps = 0

    wall_hit_total = 0
    stuck_total = 0
    timeout_total = 0

    for ep in range(num_episodes):
        # íƒ€ê²Ÿ ëœë¤
        ti = random.randint(0, len(target_keys) - 1)
        target_key = target_keys[ti]

        # ì‹œì‘ ìœ„ì¹˜ ì„¤ì •
        state = env.reset(target_idx=ti, random_start=random_start)

        done = False
        info = {}
        steps = 0

        # ì—í”¼ì†Œë“œ ë‚´ ìƒíƒœ ì¶”ì 
        hit_wall_ep = False
        stuck_ep = False
        traj = [env.pos]  # ê²½ë¡œ ê¸°ë¡ (Stuck íŒì •ìš©)

        while not done and steps < env.max_steps:
            state_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
            with torch.no_grad():
                a = int(policy(state_t).argmax().item())

            state, reward, done, info = env.step(a)
            steps += 1
            traj.append(env.pos)

            # env_advanced.pyì˜ ì‹¤ì œ ë¦¬í„´ê°’ê³¼ ë§¤ì¹­ (ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ìˆ˜ë™ ì²´í¬)
            # ë§Œì•½ envì—ì„œ infoì— hit_wallì„ ì•ˆ ì£¼ë©´, ì¢Œí‘œ ë³€í™” ì—†ìŒìœ¼ë¡œ ì²´í¬ ê°€ëŠ¥
            if info.get("hit_wall", False) or (len(traj) > 1 and traj[-1] == traj[-2]):
                hit_wall_ep = True  # ì œìë¦¬ ê±¸ìŒì´ë©´ ë²½ ì¶©ëŒë¡œ ê°„ì£¼

        # ì¢…ë£Œ í›„ ë¶„ì„
        reached = info.get("reached_goal", False)

        # Stuck íŒì • (4ë²ˆ ì™•ë³µ)
        if len(traj) > 4:
            p1, p2, p3, p4 = traj[-4:]
            if p1 == p3 and p2 == p4 and not reached:
                stuck_ep = True

        # Timeout íŒì •
        is_timeout = False
        if (not reached) and (steps >= env.max_steps):
            is_timeout = True

        # ----- í†µê³„ ì§‘ê³„ -----
        total_steps += steps
        if reached:
            total_success += 1
            total_success_steps += steps
        else:
            total_fail_steps += steps

        if hit_wall_ep: wall_hit_total += 1
        if stuck_ep: stuck_total += 1
        if is_timeout: timeout_total += 1

        # íƒ€ê²Ÿë³„ ì§‘ê³„
        tg = per_target[target_key]
        tg["total"] += 1
        tg["steps"] += steps
        if reached:
            tg["success"] += 1
            tg["success_steps"] += steps
        else:
            tg["fail_steps"] += steps
        if hit_wall_ep: tg["wall_hit_episodes"] += 1
        if stuck_ep: tg["stuck_episodes"] += 1
        if is_timeout: tg["timeout_episodes"] += 1

        # ì§„í–‰ ìƒí™© ì¶œë ¥
        if (ep + 1) % 50 == 0:
            print(f"   -> {ep + 1}/{num_episodes} ì™„ë£Œ (í˜„ì¬ ì„±ê³µë¥ : {total_success / (ep + 1) * 100:.1f}%)")

    # ----- ê²°ê³¼ ìš”ì•½ ì¶œë ¥ -----
    success_rate = total_success / num_episodes * 100.0
    avg_steps_all = total_steps / num_episodes if num_episodes > 0 else 0.0
    avg_steps_success = total_success_steps / total_success if total_success > 0 else 0.0

    wall_rate = wall_hit_total / num_episodes * 100.0
    stuck_rate = stuck_total / num_episodes * 100.0
    timeout_rate = timeout_total / num_episodes * 100.0

    print(f"\nğŸ† ì´ ì„±ê³µë¥ : {total_success}/{num_episodes} ({success_rate:.1f}%)")
    print(f"ğŸ“ í‰ê·  ìŠ¤í… (ì„±ê³µ ì‹œ): {avg_steps_success:.1f}")
    print(f"ğŸ§± Wall-hit Rate: {wall_rate:.1f}%")
    print(f"ğŸ” Stuck Rate: {stuck_rate:.1f}%")
    print(f"â± Timeout Rate: {timeout_rate:.1f}%")

    print("\nğŸ” íƒ€ê²Ÿë³„ ìƒì„¸ í†µê³„:")
    for k in target_keys:
        tg = per_target[k]
        tot = tg["total"]
        suc = tg["success"]
        rate = suc / tot * 100.0 if tot > 0 else 0.0
        avg_s = tg["steps"] / tot if tot > 0 else 0.0
        print(f"  - Target {k}: {suc}/{tot} ({rate:.1f}%) | AvgStep: {avg_s:.1f} | Stuck: {tg['stuck_episodes']}")

    print("==================================\n")


# -------------------------------------------------------------
# Main
# -------------------------------------------------------------
def main():
    # ì¬í˜„ì„± ì‹œë“œ
    random.seed(42)
    np.random.seed(42)
    torch.manual_seed(42)

    env = LibraryShelfEnvAG()
    policy = load_policy(env)

    if policy:
        # 1. ê³ ì • ì¶œë°œ (S)
        eval_mode(env, policy, num_episodes=200, random_start=False)

        # 2. ëœë¤ ì¶œë°œ (Random)
        eval_mode(env, policy, num_episodes=200, random_start=True)


if __name__ == "__main__":
    main()