# advanced_eval_summary.py
import math
import random
from collections import defaultdict

import numpy as np
import torch
import torch.nn as nn

from env_advanced import LibraryShelfEnvAG, DuelingDQN, device

MODEL_PATH = "library_AG_double_dueling.pt"

# ìŠ¬ë¼ì´ë“œìš© seed (ëŒ€í‘œê°’ìš©) + ì‹ ë¢°êµ¬ê°„ ê³„ì‚°ìš© seed ëª©ë¡
REPRESENTATIVE_SEED = 42
SEEDS_FOR_CI = [1, 42, 2025]
N_EPISODES = 200  # seedë‹¹ í‰ê°€ ì—í”¼ì†Œë“œ ìˆ˜


# --------------------------------------------------
# 1. ëª¨ë¸ ë¡œë“œ
# --------------------------------------------------
def load_policy(env):
    state_dim = env.state_dim
    n_actions = 4

    policy = DuelingDQN(state_dim, n_actions).to(device)

    ckpt = torch.load(MODEL_PATH, map_location=device)
    # state_dictë§Œ ì €ì¥í–ˆëŠ”ì§€, dictë¡œ ì €ì¥í–ˆëŠ”ì§€ ë‘˜ ë‹¤ ëŒ€ì‘
    if isinstance(ckpt, dict) and "state_dict" in ckpt:
        policy.load_state_dict(ckpt["state_dict"])
    else:
        policy.load_state_dict(ckpt)

    policy.eval()
    return policy


# --------------------------------------------------
# 2. ì—í”¼ì†Œë“œ ì‹¤í–‰ & í‰ê°€ í•¨ìˆ˜
# --------------------------------------------------
def run_episode(env, policy, random_start=False, target_idx=None):
    if target_idx is None:
        # íƒ€ê²Ÿ ëœë¤ ì„ íƒ
        target_idx = random.randint(0, len(env.target_keys) - 1)

    s = env.reset(target_idx=target_idx, random_start=random_start)

    done = False
    steps = 0
    info = {}

    while not done and steps < env.max_steps:
        state_t = torch.tensor(s, dtype=torch.float32, device=device).unsqueeze(0)
        with torch.no_grad():
            q = policy(state_t)[0]
        a = int(q.argmax().item())

        s, r, done, info = env.step(a)
        steps += 1

    reached = info.get("reached_goal", False)
    return reached, steps, target_idx


def eval_over_episodes(env, policy, n_episodes=200, random_start=False):
    """
    S-start / Random-start ê³µìš© í‰ê°€ í•¨ìˆ˜
    - ì „ì²´ ì„±ê³µë¥ , ì„±ê³µ ì‹œ í‰ê·  ìŠ¤í…
    - íƒ€ê²Ÿë³„ ì„±ê³µë¥  (ìŠ¬ë¼ì´ë“œì—ì„œ í•„ìš”í•˜ë©´ ì“°ë©´ ë¨)
    """
    target_keys = env.target_keys

    total_success = 0
    total_success_steps = 0

    per_target = defaultdict(lambda: {"total": 0, "success": 0})

    for _ in range(n_episodes):
        reached, steps, ti = run_episode(env, policy, random_start=random_start)
        key = target_keys[ti]

        per_target[key]["total"] += 1
        if reached:
            per_target[key]["success"] += 1
            total_success += 1
            total_success_steps += steps

    success_rate = total_success / n_episodes * 100.0
    avg_steps_success = (
        total_success_steps / total_success if total_success > 0 else 0.0
    )

    target_success = {}
    for k in target_keys:
        tot = per_target[k]["total"]
        suc = per_target[k]["success"]
        rate = suc / tot * 100.0 if tot > 0 else 0.0
        target_success[k] = rate

    return success_rate, avg_steps_success, target_success


# --------------------------------------------------
# 3. seedë³„ Random-start ì„±ê³µë¥ ë¡œë¶€í„° ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
# --------------------------------------------------
def eval_random_start_over_seeds():
    random_rates = []

    for sd in SEEDS_FOR_CI:
        random.seed(sd)
        np.random.seed(sd)
        torch.manual_seed(sd)
        if device.type == "cuda":
            torch.cuda.manual_seed_all(sd)

        env = LibraryShelfEnvAG()
        policy = load_policy(env)

        r_rate, r_steps, _ = eval_over_episodes(
            env, policy, n_episodes=N_EPISODES, random_start=True
        )
        random_rates.append(r_rate)
        print(f"[CIìš© Seed {sd}] Random-start ì„±ê³µë¥ : {r_rate:.1f}%")

    random_rates = np.array(random_rates)
    mean = float(random_rates.mean())
    std = float(random_rates.std(ddof=1))  # sample std
    ci_half = 1.96 * std  # Nì´ ì‘ì•„ì„œ t ëŒ€ì‹  ê·¸ëƒ¥ 1.96 ì‚¬ìš© (Curriculumê³¼ ë™ì¼ ë°©ì‹)

    return mean, std, ci_half, random_rates


# --------------------------------------------------
# 4. ë©”ì¸: ëŒ€í‘œ seedì— ëŒ€í•œ ìŠ¬ë¼ì´ë“œ ìˆ«ì + CI ê³„ì‚°
# --------------------------------------------------
def main():
    # 1) ëŒ€í‘œ seed ê¸°ì¤€ S-start / Random-start ì„±ëŠ¥ (ìŠ¬ë¼ì´ë“œ ìƒë‹¨ ë°•ìŠ¤ìš©)
    print("======================================")
    print(f"ğŸ¯ Representative Seed = {REPRESENTATIVE_SEED}")
    print("======================================")

    random.seed(REPRESENTATIVE_SEED)
    np.random.seed(REPRESENTATIVE_SEED)
    torch.manual_seed(REPRESENTATIVE_SEED)
    if device.type == "cuda":
        torch.cuda.manual_seed_all(REPRESENTATIVE_SEED)

    env = LibraryShelfEnvAG()
    policy = load_policy(env)

    s_rate, s_steps, _ = eval_over_episodes(
        env, policy, n_episodes=N_EPISODES, random_start=False
    )
    r_rate, r_steps, target_success = eval_over_episodes(
        env, policy, n_episodes=N_EPISODES, random_start=True
    )

    print("\n[Success Rate]")
    print(f"  â€¢ S-start   : ì„±ê³µë¥  {s_rate:.1f}%")
    print(f"  â€¢ Random-start : ì„±ê³µë¥  {r_rate:.1f}%")

    print("\n[Average Steps]")
    print(f"  â€¢ S-start   : {s_steps:.1f} í‰ê·  ìŠ¤í…")
    print(f"  â€¢ Random-start : {r_steps:.1f} í‰ê·  ìŠ¤í…")

    print("\n[ëœë¤ ì‹œì‘ ì‹œ íƒ€ê²Ÿë³„ ì„±ê³µë¥ ]")
    for k in env.target_keys:
        print(f"  - {k} : {target_success[k]:.1f}%")

    # 2) seedë³„ Random-start ì„±ê³µë¥  ê¸°ë°˜ ì‹ ë¢°êµ¬ê°„
    print("\n======================================")
    print("ğŸ“Š Random-start seed ë³€í™” ì‹¤í—˜ (CI ê³„ì‚°ìš©)")
    print("======================================")

    mean, std, ci_half, rates = eval_random_start_over_seeds()

    print("\n[Random-start ì‹ ë¢°êµ¬ê°„(Seed ê¸°ë°˜)]")
    print(f"  â€¢ í‰ê·    : {mean:.2f}%")
    print(f"  â€¢ í‘œì¤€ í¸ì°¨ : {std:.2f}")
    print(f"  â€¢ 95% CI : {mean:.2f} Â± {ci_half:.2f} %")
    print(f"    (seedë³„ ê°’: {', '.join(f'{r:.1f}' for r in rates)})")


if __name__ == "__main__":
    main()
