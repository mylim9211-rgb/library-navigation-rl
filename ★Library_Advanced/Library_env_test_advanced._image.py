# test_advanced.py
# ==========================================
# ğŸš¨ í° í™”ë©´ ë°©ì§€ ì½”ë“œ (ë§¨ ìœ„ì— ìˆì–´ì•¼ í•¨)
# ==========================================
import matplotlib
try:
    matplotlib.use("TkAgg")
except:
    pass
# ==========================================

import os
import time
import random
import torch
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import colors

from env_advanced import LibraryShelfEnvAG, DQN, DuelingDQN, device

# í•™ìŠµ í›„ ìƒì„±ëœ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (Double + Dueling ë²„ì „ ì¶”ì²œ)
MODEL_PATH = "library_AG_double_dueling.pt"


def load_policy(env):
    if not os.path.exists(MODEL_PATH):
        print(f"ğŸš¨ ëª¨ë¸ íŒŒì¼({MODEL_PATH})ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € train_advanced.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”!")
        return None

    ckpt = torch.load(MODEL_PATH, map_location=device)
    use_dueling = ckpt.get("use_dueling", False)
    state_dim = env.state_dim
    n_actions = 4

    if use_dueling:
        policy = DuelingDQN(state_dim, n_actions).to(device)
    else:
        policy = DQN(state_dim, n_actions).to(device)

    policy.load_state_dict(ckpt["state_dict"])
    policy.eval()
    print(f"ğŸ“¦ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {MODEL_PATH} (use_dueling={use_dueling})")
    return policy


def render_step(env, traj, ax):
    """
    env ìƒíƒœì™€ í˜„ì¬ê¹Œì§€ì˜ trajë¥¼ ì´ìš©í•´,
    ë¡œë´‡ì´ ì„œê°€ë¥¼ í–¥í•´ 'ê¸¸ì„ ì°¾ì•„ê°€ëŠ”' ëª¨ìŠµì„ í•œ í”„ë ˆì„ ê·¸ë ¤ì¤Œ.
    """
    visual = env.base_map.copy()

    # íƒ€ê²Ÿ ì„œê°€ ê°•ì¡° (ê°’ 4)
    key = env.current_target_key
    for tx, ty in env.targets[key]:
        visual[ty, tx] = 4

    # ìƒ‰ ì„¤ì •: 0 ë°”ë‹¥, 1 ë²½, 2 ì„œê°€, 3 ë²¤ì¹˜, 4 íƒ€ê²Ÿì„œê°€
    cmap = colors.ListedColormap([
        "#e0e0e0",  # 0 ë°”ë‹¥
        "#000000",  # 1 ë²½/ì¥ì• ë¬¼
        "#8B4513",  # 2 ì¼ë°˜ ì„œê°€
        "#d17f00",  # 3 ë²¤ì¹˜
        "#4B0082",  # 4 íƒ€ê²Ÿ ì„œê°€
    ])

    ax.clear()
    ax.imshow(visual, cmap=cmap, origin="upper", vmin=0, vmax=4)

    if len(traj) > 0:
        xs = [p[0] for p in traj]
        ys = [p[1] for p in traj]

        # ì§€ë‚˜ì˜¨ ê²½ë¡œ (ì–‡ì€ ì„ )
        ax.plot(xs, ys, "-", linewidth=2, color="cyan", label="Path")

        # ì‹œì‘ ì§€ì  (ë…¹ìƒ‰)
        ax.scatter(xs[0], ys[0], c="green", s=80, label="Start", zorder=5)

        # í˜„ì¬ ìœ„ì¹˜ (íŒŒë€ìƒ‰ ì )
        ax.scatter(xs[-1], ys[-1], c="blue", s=80, label="Robot", zorder=6)

    ax.set_title(f"Target: {env.current_target_key} | Steps: {env.steps}")
    ax.set_xticks([])
    ax.set_yticks([])
    ax.legend(loc="upper right")


def test(num_episodes=5, random_start=True, sleep_time=0.05):
    env = LibraryShelfEnvAG()
    policy = load_policy(env)
    if policy is None:
        return

    plt.ion()
    fig, ax = plt.subplots(figsize=(10, 4))
    plt.show(block=False)

    success_count = 0

    start_mode = "ëœë¤ ì‹œì‘" if random_start else "Sì—ì„œ ì‹œì‘"
    print(f"\nğŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘! (ì—í”¼ì†Œë“œ {num_episodes}íŒ, {start_mode})")

    for ep in range(num_episodes):
        # íƒ€ê²Ÿë„ ëœë¤ìœ¼ë¡œ í•˜ë‚˜ ë½‘ì•„ì„œ (A~G)
        target_idx = random.randint(0, len(env.target_keys) - 1)
        state = env.reset(target_idx=target_idx, random_start=random_start)

        traj = [env.pos]
        done = False
        info = {}
        steps = 0

        print(f"\nâ–¶ Episode {ep+1}/{num_episodes} | Target: {env.current_target_key}")

        while not done and steps < env.max_steps:
            # ì‹œê°í™”: ë¡œë´‡ì´ í•œ ì¹¸ì”© ì›€ì§ì´ëŠ” ëª¨ìŠµ
            render_step(env, traj, ax)
            fig.canvas.draw()
            fig.canvas.flush_events()
            time.sleep(sleep_time)

            state_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
            with torch.no_grad():
                q = policy(state_t)[0]
            action = int(q.argmax().item())

            state, reward, done, info = env.step(action)
            steps += 1

            if not traj or traj[-1] != env.pos:
                traj.append(env.pos)

        # ë§ˆì§€ë§‰ í”„ë ˆì„ í•œ ë²ˆ ë” ê·¸ë¦¬ê¸°
        render_step(env, traj, ax)
        fig.canvas.draw()
        fig.canvas.flush_events()
        time.sleep(0.2)

        if info.get("reached_goal", False):
            success_count += 1
            print(f"   âœ” ì„±ê³µ! (steps={steps})")
        else:
            print(f"   âœ– ì‹¤íŒ¨.. (steps={steps})")

        # í•œ ì—í”¼ì†Œë“œ ëë‚  ë•Œë§ˆë‹¤ ì ê¹ ë©ˆì¶¤
        time.sleep(0.5)

    plt.ioff()
    plt.show()

    print("\n=====================================")
    print(f"ğŸ¯ ì´ {num_episodes}íŒ ì¤‘ {success_count}íŒ ì„±ê³µ")
    print(f"ğŸ”¥ ì„±ê³µë¥ : {success_count/num_episodes*100:.1f}%")
    print("=====================================\n")


if __name__ == "__main__":
    # ê¸°ë³¸: ëœë¤ ì‹œì‘ í™˜ê²½ì—ì„œ 5íŒ ì •ë„ ê¸¸ì°¾ê¸° ì‹œì—°
    test(num_episodes=5, random_start=True, sleep_time=0.05)

    # Sì—ì„œë§Œ ì‹œì‘í•˜ëŠ” ë²„ì „ë„ ë³´ê³  ì‹¶ìœ¼ë©´:
    # test(num_episodes=5, random_start=False, sleep_time=0.05)
