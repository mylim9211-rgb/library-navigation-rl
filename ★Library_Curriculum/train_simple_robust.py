import random
import numpy as np
from collections import deque

import torch
import torch.nn as nn
import torch.optim as optim

# ğŸ’¡ í™˜ê²½ íŒŒì¼ëª…ì„ env_curriculumìœ¼ë¡œ ìˆ˜ì •í•˜ì—¬ ì„í¬íŠ¸
from env_curriculum import LibraryShelfEnvAG

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

MODEL_PATH = "library_simple_robust.pt"


# ----------------------------------------------------
# 1. Standard DQN (Step 2 ì „ìš© ëª¨ë¸)
# - Random Start ìƒí™©ì—ì„œ ìœ„ì¹˜ì™€ íƒ€ê²Ÿì˜ ìƒê´€ê´€ê³„ë¥¼ ì¶©ë¶„íˆ í•™ìŠµí•  ìˆ˜ ìˆëŠ” 128 ë…¸ë“œ êµ¬ì„±
# ----------------------------------------------------
class StandardDQN(nn.Module):
    def __init__(self, state_dim, n_actions):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, n_actions),
        )

    def forward(self, x):
        return self.net(x)


# ----------------------------------------------------
# 2. Replay Buffer
# ----------------------------------------------------
class ReplayBuffer:
    def __init__(self, capacity=50000):
        self.buffer = deque(maxlen=capacity)

    def push(self, s, a, r, ns, d):
        self.buffer.append((s, a, r, ns, d))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        s, a, r, ns, d = zip(*batch)
        return (
            torch.tensor(np.array(s), dtype=torch.float32, device=device),
            torch.tensor(a, dtype=torch.long, device=device).unsqueeze(1),
            torch.tensor(r, dtype=torch.float32, device=device).unsqueeze(1),
            torch.tensor(np.array(ns), dtype=torch.float32, device=device),
            torch.tensor(d, dtype=torch.float32, device=device).unsqueeze(1),
        )

    def __len__(self):
        return len(self.buffer)


# ----------------------------------------------------
# 3. í•™ìŠµ ë©”ì¸ ë¡œì§
# ----------------------------------------------------
def main():
    # --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ---
    EPISODES = 2000  # ì¼ë°˜í™” ì„±ëŠ¥(Random Start) í™•ë³´ë¥¼ ìœ„í•´ 2000íšŒ ìˆ˜í–‰
    BATCH_SIZE = 64
    LR = 0.0005  # ì ì ˆí•œ í•™ìŠµë¥ 
    GAMMA = 0.99
    TARGET_UPDATE_FREQ = 200  # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ ì£¼ê¸°

    epsilon = 1.0
    epsilon_min = 0.05
    epsilon_decay = 0.995

    env = LibraryShelfEnvAG()
    state_dim = env.state_dim
    n_actions = env.n_actions

    # ëª¨ë¸ ìƒì„± (Standard DQN)
    policy_net = StandardDQN(state_dim, n_actions).to(device)
    target_net = StandardDQN(state_dim, n_actions).to(device)
    target_net.load_state_dict(policy_net.state_dict())

    optimizer = optim.Adam(policy_net.parameters(), lr=LR)
    memory = ReplayBuffer()

    print(f"ğŸš€ [Step 2: Simple Robust] í•™ìŠµ ì‹œì‘ (Device: {device})")
    print("ğŸ‘‰ í™˜ê²½ íŒŒì¼: env_curriculum.py ì‚¬ìš©")
    print("ğŸ‘‰ ì „ëµ: Random Start 50% í™•ë¥  ì ìš© + Double DQN ë¡œì§")

    rewards_history = []

    for ep in range(EPISODES):
        # ğŸ’¡ ì¼ë°˜í™” í…ŒìŠ¤íŠ¸: 50% í™•ë¥ ë¡œ ëœë¤ ì‹œì‘
        use_random_start = (random.random() < 0.5)

        state = env.reset(random_start=use_random_start)
        total_reward = 0
        done = False

        while not done:
            # Action ì„ íƒ
            if random.random() < epsilon:
                action = random.randint(0, n_actions - 1)
            else:
                with torch.no_grad():
                    q = policy_net(torch.tensor(state, dtype=torch.float32, device=device))
                    action = int(q.argmax().item())

            # Step
            next_state, reward, done, info = env.step(action)

            memory.push(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward

            # í•™ìŠµ ìˆ˜í–‰ (ë°ì´í„° ì¶•ì  í›„)
            if len(memory) > 1000:
                s_b, a_b, r_b, ns_b, d_b = memory.sample(BATCH_SIZE)

                # [Double DQN ì ìš©]
                with torch.no_grad():
                    # ì°¨ê¸° í–‰ë™ ì„ íƒ: Policy Net
                    next_actions = policy_net(ns_b).argmax(dim=1, keepdim=True)
                    # ê°€ì¹˜ í‰ê°€: Target Net
                    next_q = target_net(ns_b).gather(1, next_actions)
                    target = r_b + GAMMA * next_q * (1 - d_b)

                current_q = policy_net(s_b).gather(1, a_b)

                # ì•ˆì •ì ì¸ ìˆ˜ë ´ì„ ìœ„í•œ SmoothL1Loss ì‚¬ìš©
                loss = nn.SmoothL1Loss()(current_q, target)

                optimizer.zero_grad()
                loss.backward()
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ìœ¼ë¡œ ì•ˆì •ì„± ê°•í™”
                nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)
                optimizer.step()

        # Epsilon Decay
        if epsilon > epsilon_min:
            epsilon *= epsilon_decay

        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ë™ê¸°í™”
        if ep % TARGET_UPDATE_FREQ == 0:
            target_net.load_state_dict(policy_net.state_dict())

        rewards_history.append(total_reward)

        # ë¡œê·¸ ì¶œë ¥ (100 ì—í”¼ì†Œë“œ ë‹¨ìœ„)
        if (ep + 1) % 100 == 0:
            avg_r = np.mean(rewards_history[-100:])
            print(
                f"Ep {ep + 1:4d} | Avg Score: {avg_r:6.2f} | Eps: {epsilon:.2f} | Mode: {'Random' if use_random_start else 'Fixed'}")

    # ìµœì¢… ëª¨ë¸ ì €ì¥
    torch.save(policy_net.state_dict(), MODEL_PATH)
    print(f"\nâœ… Step 2 í•™ìŠµ ì¢…ë£Œ! ëª¨ë¸ ì €ì¥ë¨: {MODEL_PATH}")


if __name__ == "__main__":
    main()