# curriculum_eval_visual.py
import random
from collections import Counter

import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

from env_curriculum import LibraryShelfEnvAG  # ë„ˆê°€ ì“°ëŠ” Curriculum Grid í™˜ê²½

# ğŸ”¹ í•œê¸€ í°íŠ¸ ì„¤ì • (Windows ê¸°ì¤€)
plt.rcParams["font.family"] = "Malgun Gothic"
plt.rcParams["axes.unicode_minus"] = False

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ğŸ”¹ Robust / Baseline ëª¨ë¸ ê²½ë¡œë§Œ ë°”ê¿”ê°€ë©´ì„œ ì“°ë©´ ë¨
MODEL_PATH = "library_simple_robust.pt"      # Robust
# MODEL_PATH = "library_curriculum_base.pt"  # Baseline í‰ê°€í•˜ê³  ì‹¶ìœ¼ë©´ ì´ê±¸ë¡œ êµì²´


# --------------------------------------------------
# 1. ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° (Robust í•™ìŠµ ë•Œ ì¼ë˜ StandardDQNê³¼ ë™ì¼)
# --------------------------------------------------
class StandardDQN(nn.Module):
    def __init__(self, state_dim, n_actions):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, n_actions),
        )

    def forward(self, x):
        return self.net(x)


# --------------------------------------------------
# 2. ëª¨ë¸ ë¡œë“œ
# --------------------------------------------------
def load_policy(model_path=MODEL_PATH):
    env = LibraryShelfEnvAG()
    state_dim = env.state_dim
    n_actions = env.n_actions

    policy = StandardDQN(state_dim, n_actions).to(device)
    state_dict = torch.load(model_path, map_location=device)
    policy.load_state_dict(state_dict)
    policy.eval()

    print(f"ğŸ“¦ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
    return env, policy


# --------------------------------------------------
# 3. ì—í”¼ì†Œë“œ ì‹¤í–‰ í•¨ìˆ˜ (greedy ì •ì±…)
# --------------------------------------------------
def run_episode(env, policy, random_start=False, target_idx=None):
    s = env.reset(random_start=random_start, target_idx=target_idx)

    done = False
    steps = 0
    reached = False

    while not done and steps < env.max_steps:
        state_t = torch.tensor(s, dtype=torch.float32, device=device).unsqueeze(0)
        with torch.no_grad():
            q = policy(state_t)[0]
        a = int(q.argmax().item())

        s, r, done, info = env.step(a)
        steps += 1

        if info.get("reached_goal", False):
            reached = True

    return reached, steps


# --------------------------------------------------
# 4. S-start vs Random-start ì„±ëŠ¥ í‰ê°€
# --------------------------------------------------
def eval_start_condition(env, policy, n_episodes=200):
    results = {}

    for mode_name, random_flag in [("S-start", False), ("Random-start", True)]:
        success = 0
        steps_success = []

        for _ in range(n_episodes):
            reached, steps = run_episode(env, policy, random_start=random_flag)
            if reached:
                success += 1
                steps_success.append(steps)

        success_rate = success / n_episodes * 100.0
        avg_steps = np.mean(steps_success) if steps_success else None
        results[mode_name] = (success_rate, avg_steps)

        if avg_steps is not None:
            print(f"[{mode_name}] ì„±ê³µë¥ : {success_rate:.1f}% | "
                  f"ì„±ê³µ ì‹œ í‰ê·  ìŠ¤í…: {avg_steps:.1f}")
        else:
            print(f"[{mode_name}] ì„±ê³µë¥ : {success_rate:.1f}% | ì„±ê³µ ì—í”¼ì†Œë“œ ì—†ìŒ")

    return results


# --------------------------------------------------
# 5. íƒ€ê²Ÿë³„ ì„±ê³µë¥  (ëœë¤ ì‹œì‘ ê¸°ì¤€)
# --------------------------------------------------
def eval_targetwise(env, policy, n_episodes_per_target=50):
    target_keys = env.target_keys  # ['A','B','C','D','E','F'] ê°™ì€ êµ¬ì¡°ë¼ê³  ê°€ì •
    success_dict = {}

    for idx, key in enumerate(target_keys):
        success = 0
        for _ in range(n_episodes_per_target):
            reached, steps = run_episode(
                env,
                policy,
                random_start=True,
                target_idx=idx
            )
            if reached:
                success += 1
        rate = success / n_episodes_per_target * 100.0
        success_dict[key] = rate
        print(f"íƒ€ê²Ÿ {key}: ì„±ê³µë¥  {rate:.1f}%")

    return success_dict


# --------------------------------------------------
# 6. ë°” ì°¨íŠ¸ ì‹œê°í™” (S vs Random, íƒ€ê²Ÿë³„ ì„±ê³µë¥ )
# --------------------------------------------------
def plot_summary(start_results, target_success):
    # (1) S-start vs Random-start
    labels = list(start_results.keys())
    rates = [start_results[k][0] for k in labels]

    plt.figure(figsize=(5, 4))
    plt.bar(labels, rates)
    for i, v in enumerate(rates):
        plt.text(i, v + 1, f"{v:.1f}%", ha="center")
    plt.ylim(0, 100)
    plt.ylabel("Success Rate (%)")
    plt.title("Curriculum Grid â€“ S ì‹œì‘ vs Random ì‹œì‘ ì„±ê³µë¥ ")
    plt.tight_layout()
    plt.show()

    # (2) íƒ€ê²Ÿë³„ ì„±ê³µë¥ 
    keys = list(target_success.keys())
    vals = [target_success[k] for k in keys]

    plt.figure(figsize=(6, 4))
    plt.bar(keys, vals)
    for i, v in enumerate(vals):
        plt.text(i, v + 1, f"{v:.1f}%", ha="center")
    plt.ylim(0, 100)
    plt.ylabel("Success Rate (%)")
    plt.title("ëœë¤ ì‹œì‘ ì‹œ íƒ€ê²Ÿë³„ ì„±ê³µë¥ ")
    plt.tight_layout()
    plt.show()


# --------------------------------------------------
# 7. ë©”ì¸
# --------------------------------------------------
if __name__ == "__main__":
    env, policy = load_policy()

    print("\n=== [1] S-start vs Random-start í‰ê°€ ===")
    start_results = eval_start_condition(env, policy, n_episodes=200)

    print("\n=== [2] íƒ€ê²Ÿë³„ ì„±ê³µë¥  í‰ê°€ (ëœë¤ ì‹œì‘) ===")
    target_success = eval_targetwise(env, policy, n_episodes_per_target=50)

    print("\n=== [3] ìš”ì•½ ê·¸ë˜í”„ ì¶œë ¥ ===")
    plot_summary(start_results, target_success)

    print("\nâœ… Curriculum Grid Evaluation & Visualization ì™„ë£Œ")
